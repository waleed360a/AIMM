import streamlit as st
import time  # For loading animation

# Import your RAG system components
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import HuggingFaceHub
import os


# Streamlit Web App
st.set_page_config(page_title="Ù…Ø³Ø§Ø¹Ø¯ Ù…ÙˆÙ†ÙŠ Ù…ÙˆÙ†", page_icon="ğŸ’¬", layout="centered")

# Header
st.title("ğŸ’¬ Ù…Ø³Ø§Ø¹Ø¯ Ù…ÙˆÙ†ÙŠ Ù…ÙˆÙ†")
st.write("Ø£Ù‡Ù„Ø§ ÙˆØ³Ù‡Ù„Ø§ Ø¨Ùƒ ÙÙŠ Ù…Ø³Ø§Ø¹Ø¯ **Ù…ÙˆÙ†ÙŠ Ù…ÙˆÙ†**! Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ Ø­ÙˆÙ„ Ø®Ø¯Ù…Ø§ØªÙ†Ø§ ÙˆØ³ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø©. ğŸ˜Š")
huggingface_api = st.text_input("api Ø§Ø¯Ø®Ù„", placeholder="Ø§ÙƒØªØ¨ Ù‡Ù†Ø§...")
apiButton= st.button("Ø§Ø¶ØºØ· Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯Ù„")
# Input Area
question = st.text_input("ğŸ“ Ø§Ø³Ø£Ù„Ù†ÙŠ Ø³Ø¤Ø§Ù„", placeholder="Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...")
model='mistralai/Mistral-7B-Instruct-v0.3'

if apiButton:
    @st.cache_resource
    def load_llm():
        return HuggingFaceHub(
            repo_id=model,
            model_kwargs={"temperature": 0.1, 'stream':True},
            huggingfacehub_api_token=huggingface_api,
        )

    llm = load_llm()
    model_name= 'Linq-AI-Research/Linq-Embed-Mistral'
    # Load embeddings using HuggingFaceEmbeddings
    @st.cache_resource
    def load_embeddings():
        return HuggingFaceEmbeddings(model_name='Linq-AI-Research/Linq-Embed-Mistral')

    embeddings = load_embeddings()

    # Cache ChromaDB for improved performance
    @st.cache_resource
    def load_chroma():
        return Chroma(
            embedding_function=embeddings,
            persist_directory='./chroma_db'  # Load from disk
        )

    vectordb = load_chroma()
    # Create a retriever for querying the vector database
    retriever = vectordb.as_retriever(score_threshold= 1)


    system_prompt = """
    Given the following context, generate an answer based on this context only. Don't try to make up an answer.
    In the reply, try to provide as much text as possible from the "Answer" section in the source document context without making many changes.
    If the answer is not found in the context, state "Ù…Ø§ Ø¹Ù†Ø¯Ù†Ø§ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø¨Ø­ÙŠØ« Ù†Ù‚Ø¯Ø± Ù†Ø®Ø¯Ù…Ùƒ. Ù†Ø¹ØªØ°Ø±."
    CONTEXT: {context}
    """

    # Create the prompt template
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "Ø§Ù„Ø³Ø¤Ø§Ù„: {input}\n\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")  # Improves formatting
        ]
    )

    # Create the document combination chain
    question_answer_chain = create_stuff_documents_chain(llm, prompt)

    # Create the retrieval chain
    chain = create_retrieval_chain(retriever, question_answer_chain)
    def get_answer(question):
        """
        Queries the retrieval chain with a given question and extracts the clean answer.

        Args:
            question (str): The user's question.

        Returns:
            None (prints the answer directly).
        """
        try:
            # Invoke the retrieval chain with the question
            response = chain.invoke({"input": question})
            
            # Check if the response contains the expected output, assuming it's a dictionary
            if 'answer' in response:
                # Remove unnecessary context
                clean_answer = response["answer"]
                if "Human: Ø§Ù„Ø³Ø¤Ø§Ù„:" in clean_answer:
                    clean_answer = clean_answer.split("Human: Ø§Ù„Ø³Ø¤Ø§Ù„:")[-1].strip()
                if "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:" in clean_answer:
                    clean_answer = clean_answer.split("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")[-1].strip()
                
                if "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª" in clean_answer:
                    clean_answer = "Ù…Ø§ Ø¹Ù†Ø¯Ù†Ø§ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø¨Ø­ÙŠØ« Ù†Ù‚Ø¯Ø± Ù†Ø®Ø¯Ù…Ùƒ. Ù†Ø¹ØªØ°Ø±."
                
                
    #Format with the template
                template = f"""
            \n  Ù‡Ù„Ø§ ÙˆØ³Ù‡Ù„Ø§ Ø¹Ù…ÙŠÙ„Ù†Ø§ Ø§Ù„Ø¹Ø²ÙŠØ² 
            \n  Ù…Ø¹Ø§Ùƒ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø´Ø®ØµÙŠ ÙÙŠ Ù…ÙˆÙ†ÙŠ Ù…ÙˆÙ† 
            \n {clean_answer}\n
                \nØ§ØªÙ…Ù†Ù‰ Ø§Ù† ØªÙƒÙˆÙ† Ù…Ø´ÙƒÙ„ØªÙƒ Ø§Ù†Ø­Ù„Øª ğŸ˜Š\n
                """
                
                return template.strip()  # Print the final response 
            else:
                return "No valid output received from the retrieval chain."
        
        except Exception as e:
            print(f"An error occurred: {str(e)}")


    # Answer Area
    if st.button("ğŸ” Ø§Ø³Ø£Ù„"):
        if question.strip() != "":
            # Show loading animation
            with st.spinner("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©..."):
                answer = get_answer(question)
            # Show Response
            if "Ù…Ø§ Ø¹Ù†Ø¯Ù†Ø§ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©" in answer:
                st.warning(answer)
            else:
                st.success(answer)
        else:
            st.error("â—ï¸ÙŠØ±Ø¬Ù‰ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ Ù‚Ø¨Ù„ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± 'Ø§Ø³Ø£Ù„'")
